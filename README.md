This README document has the intent to explain the logic behind
the HA Procedure that it uses the combination of Heat Scaling group,
Heat Resource Group, Ceilometer Alarm and Nova metadata.

Background and requirements: this solution has been designed
with the absolutely requirement to be agentless and to relay
only on the OpenStack environment, such as Nova and Ceilometer.
It is also important to keep consistent the number of VM created.

############# Logic work-flow #############
# TIME0
At Time0 the template has been loaded into Heat.
The heat engine processed it and the result
is what has been described into the templates.
Let's assume for example that has been created:
- One network, one related subnet, one router
- One ResourceGroup policy that has defined 20...
- ...20 "OS::Scaling::Fedora". This resource have the following content
  - One Randon String
  - One AutoScalingGroup with "OS::VM::Fedora" (min 1, max 2/3)
  - The VM "OS::VM::Fedora" have a custom metadata "metering.stack" 
  - One ScalingPolicy (+1 in size)
  - One BackToNormalPolicy (-1 in size)
  - Three OS::Ceilometer::Alarm:
    - MissingSample: meter cpu, evaluation periods 3x 60s, threshold 1, comparison_operator: lt. "insufficient_data_actions:" -> Call ScalingPolicy
    - NoExecution: meter cpu, evaluation periods 3x 60s, threshold 1, comparison_operator: lt. "alarm_actions:" -> Call ScalingPolicy
    - BackToNormal: meter cpu, evaluation periods 3x 60s, threshold 1, comparison_operator: gt. "alarm_actions:" -> Call BackToNormalPolicy

# TIME1a
At Time1 the compute host that was hosting a portion
of the previously running VM is now down.

# TIME1b (TIME1a + about 3 minutes)
At Time1b the Ceilometer alarm for each VM
(3x Ceilometer alarm for VM) will report a fault situation.
These alarms are called: NoExecution and MissingSamples.
Also a special alarm for the Time2 is now in OK status. 

# TIME1c (TIME1b + 1 minute)
At Time1c the previously triggered alarm
have created one VM in replacement of the death one.

# TIME2 (TIME1c + 3 minutes)
At Time2 the special alarm BackToNormal triggered
the deletion of the non-running VM

############# How Things work here #############
The meaning to use the Heat AutoScalingGroup is to provide a kind of VM life-cicly.
The AutoScalingGroups are generated by the ResourceGroup, which is another form the 
AutoScalingGroup without any form of knowledge of VM status.
In order to provide reliable Ceilometer alarm (which by designed look the whole tenant
and not specific resources) each ResourceGroup will create:
- one AutoScalingGroup
- this one AutoScalingGroup will manage ONLY one VM (min size 1, max size 2/3)
- The Ceilometer alarm will trigger the VM creation if the samples will disapper
- The Ceilometer alarm will trigger the VM creation if the samples will return back (by design will be deleted the oldest resources, which is the non-healy one) 

In order to have Ceilometer Alarms per VM a random string will be used segregated other tenant VMs and other AutoScalingGroup VMs.

############# Limitations #############
- In order to have a reliable Ceilometer samples the "cpu" meters has to be generated every 60 seconds
- A lost-quorum/split-brain controller situation (at least two out of three) will prevent this system to work correrly since the OpenStack API will not answer anymore

############# TODO/Improvments #############
- Implement Anti-Affinity Policy, to be erditated by the main stack to all of the VMs
- Better testing of host down (really down) scenario 
